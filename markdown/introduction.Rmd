---
title: " "
output: pdf_document
header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
---
# Introduction

## From the Central Dogma of Molecular Biology to Omics and Systems Biology
Before understanding the underlying principle biological systems undergo and the initial exploration of molecular biology, the processes of live were roughly inferred and not well understood. The central dogma of molecular biology serves as the cornerstone of biological processes, providing a framework for understanding how genetic information is converted into functional proteins. This framework consists of two major processes, which involve transcription, converting DNA into RNA, and translation, converting RNA into functional proteins. In between those processes regulation takes base on a molecular level, by altering structure with chemical adaptations. The regulation is undertaken to provide biological organisms with an highly efficient output of their  genetic products called proteins. Crick's formulation of the central dogma has guided research in molecular biology for over 60 years \citep{Cobb2017}. Over the past six decades, researchers have explored various aspects of this principle, collectively referred to as omics, to comprehensively analyze biomolecules in diverse contexts. Omics research has emerged as an approach for comprehensively studying biomolecules and their interactions within biological systems. Omics encompasses various fields, including genomics, transcriptomics, proteomics, metabolomics, and others, each focusing on a specific class of biomolecules. By analyzing multiple omics layers, researchers can gain a holistic view of biological processes, uncover regulatory mechanisms, and identify key players in complex biological networks with computational methods. Systems biology takes a quantitative approach to investigate the interactions, dynamics, and emergent properties of biomolecular networks. Imagining biological systems as complex, multi-layer networks, the ensemble of proteins, known as the proteome, plays a critical role in this interactive structure with many functions such as structural integrity, catalyzing chemical reaction and regulation of cellular funcitons.

## Proteomics
The central dogma of molecular biology offers insights not only at the level of individual terms but also provides a hierarchical understanding of biology. Hierarchical organization is the underlying principle in all topics of biology. Proteins exhibit hierarchical structures that contribute to their functionality. At the primary structure level, proteins are composed of smaller building blocks called amino acids. Combining them results in the protein secondary structure such as alpha helix and beta sheet. These secondary structures further assemble to create higher-order tertiary structures, which represent specific domains within the protein. Finally, the quaternary structure describes the functional state of the protein as a whole at a given time. For instance pathogens interact with cells of an organism a cascade of biochemical reactions takes place. These reactions influence the outcome of the cellular proteome in regards of localization, abundance, post-translational modifications and more \citep{Beltran2017}. 
By studying the proteome, which encompasses all the proteins present in a biological system at a specific moment, we can gain a comprehensive understanding of the hierarchical organization and functional dynamics of biological processes.

### Bulk proteomics
Higher organisms are composed of specialized cells organized into tissues, such as skin, muscle, and blood. Each tissue consists of cells with specific functions, resulting in variations in protein expression. Bulk proteomics is a technique used to analyze the protein composition of a tissue sample, which contains all types of cells present in that particular tissue. This approach finds valuable applications for instance in oncology, where it can provide insights into the protein profile of a tumor. By studying oncogenic biomarkers and expression patterns, this knowledge can contribute to the development of screening methods and the customization of treatments based on observed expression patterns. Additionally, bulk proteomics can aid in identifying the progression of tumor development \citep{Kwon2021}. Bulk analysis expression profiles provide scientists with an average protein abundance across all cells in a sample, offering a comprehensive view of protein expression. Additionally, compared to single-cell methods, bulk samples typically contain a greater number of proteins. In fact the expanded library can be used to validate the performance of single-cell type and single-cell proteomics methods and bioinformatics pipeline \citep{Schoof2021}. However, in this type of analysis, the precise properties of individual cell types can only be inferred and remain obscured.

### Single-cell type proteomics
Taking tissue samples can lead to an averaging effect across the entire cellular ensemble, making it difficult to discern specific cell types. To overcome this limitation, cell sorting techniques such as fluorescence-activated cell sorting (FACS), magnetic-activated cell sorting (MACS), and buoyancy-activated cell sorting (BACS) are employed \citep{Liou2015}. These techniques rely on the detection of surface proteins on cells and the specific binding of antibodies to these proteins. Antibodies, typically derived from the immune system of other species, bind to antigens in a lock-and-key manner. Antibodies are widely used in molecular biology and find applications in cell isolation. Depending on the desired application, the antibodies can be labeled with fluorescent markers (FACS), magnetic beads (MACS), or biotin (BACS). These methods have varying technical requirements, with flow cytometers being the most complex and costly. In MACS, magnetic beads are used to isolate cells from a solution by utilizing a magnet. However, mechanical forces involved in MACS can potentially damage the cells. BACS, a method developed in 2015, employs microbubbles to carry the antibodies for cellular isolation. The method showed improvements compared to MACS in regards of viability after sorting. 
It is important to note that FACS is considered the gold standard for validating the effectiveness of MACS and BACS due to its high purity and yield. FACS was used in the validation of these two methods \citep{Liou2015, Sutermaster2019}. Single-cell type proteomics reflects the protein ensemble of a specific cell type at a particular time, providing a focused perspective on the studied field compared to bulk methods \citep{Maes2020}.

### Single-cell proteomics
Cell sorting techniques primarily rely on extracellular proteins to identify and separate different cell types. However, the intracellular content of cells can still exhibit variability in experiments. Single-cell proteomics (SCP) addresses this limitation by providing a snapshot of the underlying processes within an individual cell, eliminating this variability and allowing for the elucidation of the  principles of cell type behavior, such as differentiation.
Differentiation, a critical process in various biological systems such as the immune system, involves the acquisition of specialized functions by specific cell types. Investigating differentiation in multiple cells can be challenging due to the inherent heterogeneity during the initiation of this process. However, by analyzing one cell at a time and observing it throughout the differentiation process, researchers can capture the complete trajectory of differentiation at a single-cell level. This approach provides valuable insights into the dynamic changes occurring within cells during differentiation.
In 2021, Specht et al. developed the SCoPE2 sample preparation and analysis pipeline, performing macrophage-monocyte differentiation experiments \citep{Specht2021}. This pipeline streamlined the sample preparation process, reducing labor time and providing a reference model for subsequent SCP projects. SCP has proven to be a powerful tool for studying a wide range of biological phenomena, including drug responses, infectious diseases, and organism development. Over the past decade, single-cell techniques, including SCP, have become indispensable tools for the global research community \citep{Minakshi2019}.

## Capturing the proteome
An early approach of qualitative analysis of the cellular proteome involved labeling with fluorescent antibodies and imaging. The major disadvantage of this technique was the limitation to only stain a few proteins per cell and the requirement of a-priori knowledge of the protein to analyze. For quantification procedures such as Western blots, immunoassays or cytometry by time of flight (CyTOF) have been used. Other disadvantages are the ability to permeate cells, accessibility and binding of the epitope and the creation of specific antibodies for a given protein \citep{Budnik2018}.
Early quantitative methods in molecular biology were assays like the UV-vis-, Bradford- and/or bicinchoninic-acid-assay measuring the entire protein concentration. These methods are still in use to obtain total concentration for sample preparation. 
Another quantitative technique involved RNA-sequencing. RNAseq refers to the task of examining the transcriptome of bulk tissue sample  with deep-sequencing technologies. RNA in a sample includes mRNAs, non-coding RNAs, and small RNAs. One of the main disadvantages is, that the mapping relies on the knowledge of the DNA sequence of the observed organism (reference genome) \citep{Wang2009}. Another disadvantage is, that the amount of protein translated from the determined mRNA can only be estimated \citep{Gygi1999}. However, today these predictions can be done with computational methods such as LASSO and the accuracy of the estimate highly depends on selection and validation of this method \citep{Magnusson2022}.
When observing the protein ensemble of a cellular environment with a top-down approach  like DNAseq or RNAseq, the characteristics (e.g. post-translational modifications or charge) and local concentration of the proteins remain hidden. The cause of this phenomenon is that only a fraction of the DNA is transcribed into mRNA and not all mRNA is further translated into protein. Furthermore multiple proteins can be derived from a single DNA-sequence with alternative splicing and different proteins coming from non related sequences acting like building blocks can form protein complexes and/or Protein-RNA complexes. Cells are highly efficient and do not waste energy by producing obsolete proteins, moreover the degradation of proteins is an important energy recovery mechanism. A bottom-up strategy can elucidate these properties of the protein ensemble at a given time leading to proteomics. 
When studying impact of an external signal or stressor to a cell the abundance and   properties of a protein or protein-ensemble can enlighten the underlying pathway leading to a biological response. Since some proteins do not even function before post-translational modifications (PTMs) happen, a knowledge about these can be crucial for further studies. 
Mass spectrometry (MS) fusions the advantages of  the above mentioned techniques, because it combines the specificity of qualitative assays which are even able to sense (PTMs) with the quantitative accuracy of  protein concentration assays. Furthermore MS does not require any a-priori knowledge of the protein ensemble of the specimen, making it the method of choice for untargeted approaches.

### Mass Spectrometry
Mass spectrometry enables qualitative and quantitative analysis of the entire repertoire of a biological sample. Mass spectrometers measure the mass to charge ratio (m/z) of a particle from a fragmented larger molecule. This is achieved by a physical procedure done with a device which is made of three major components: the ion source, an analyzer, and a detector. The ion sources charges molecules and accelerate them through a magnetic field. The analyzer separates particles according to their mass to charge ratio and the detector senses charged particles and amplifies their signal \citep{Parker2010}. However before acquiring any data, sample preparation and pre-processing is required.
After sample preparation the first step involves tryptic digestion. It is a widely used technique in shotgun proteomics and involves the enzymatic cleavage of proteins into smaller peptides using the protease enzyme trypsin. This process is known as proteolysis. The enyme cleaves specifically at the caroboxyl side of arginine and lysis residues. Afterwards a charge remains on the residues making them detectable for the mass spectrometer. It is proven that proteins which are digested to the limit, meaning no further proteolysis can be induced, provide the best results in such bottom up approaches \citep{Brownridge2011, Laskay2013}.

To observe multiple samples in one run, labeling is needed to identify each sample within a batch. 
Since mass spectrometry is not a quantitative technique by itself, the peak height or area does not reflect the abundance of a peptide. Physicochemical properties of the peptides can change the ionization efficiency and detectability of the target. However, when comparing the same analyte between multiple runs of labeled peptides, differences in the reporter ion intensity reflect the abundance of those. Labels should be chosen to change solely the mass of the sample and to not affect folding or other inherent properties of the peptide. The two techniques for labeling peptides are metabolic labeling and isobaric labling. In metabolic labeling the cells were fed with aminoacids containing heavy isotopes, it is the method of choice in order to label peptides at the earliest possible level. This atoms can be heavy nitrogen in aminoacids or salts in fertilizer for plants. Mass shifts are proportional to the isotopes incorporated during biomass production and are visible after proteolytic cleavage. Stable isotope labeling in cell culture (SILAC) was presented in the early 2000s. This method used heavy aminoacid enriched media to feed cells, in order to quantitatively analyze expression profiles. The limitation of this method is, that it can only be applied to a human cell culture (in vitro) or to model organisms (in vivo).
Isobaric labeling solely applies to in vitro techniques, with TMT and iTRAQ. The data analyzed by the developed pipeline primarily utilized TMT tags. Tandem mass tag (TMT) reagents enable differentiation of multiple samples in a single mass spectrometry (MS) run. Each sample is individually labeled with TMT reagents and then pooled together, this procedure is called multiplexing. TMTs have the same charge and differ only by their isotopic masses, the peaks observed for each sample are called reporter ions (RI). Each RI and sample is interpreted as one channel in downstream analysis. This technique allows for the identification, enrichment, and quantification of low-abundance peptide ions, which is particularly valuable in single-cell techniques. With this technique it is possible to quantify proteins and differ low abundant proteins from background noise. However, a drawback of isobaric labeling is the presence of co-fragmentation signals in the spectrogram, which requires data normalization to remove unwanted contributions \citep{Marx2019, Budnik2018}. Isotopic distribution in TMTs, reflecting the natural distribution, can be corrected during data acquisition by considering a defined spread in other channels. Single-cell proteomics by mass spectrometry (SCoPE-MS) uses a isobaric labeling with a tandem mass tag (TMT) to enhance signal intensity of a protein species. Loading of a single-cell channel (SCC) with the right amount can be balanced to the protein concentration of a single cell, leading to single-cell proteomics \citep{Ye2022}. 

Before ionization peptides need to be further separated according to their chemical properties, size or species by liquid chromatography (LC), because the number of peptide processed simultaneously by the MS device is limited.  In the case of mass spectrometry, liquid chromatography especially high performance LC (HPLC) is the technique of choice and replaces the instrumentation-wise simpler technique gel electrophoresis. In HPLC the molecules eluate through a narrow column with high pressures (50-350 bar), compared to LC where the separation is gravity dependent. Similar to gel electrophoresis molecules can be separated upon different properties depending on the type of the chromatography column. In a typical proteomics experiment peptides are separated with reversed phase chromatography (RPLC). The principle behind RPLC is a column with a nonpolar stationary phase which interacts with the nonpolar residues of the peptides. Peptides were fractionated according to their hydrophobicity.

To analyze a biological sample consisting of peptides in solution the liquid needs to be vaporized into gas phase. Two techniques are capable of this procedure. Electrospray ionization (ESI) pushes the analyte through a capillary while applying an electric current to the liquid, vaporizing the sample to a  charged aerosol. Peptides are further fragmented according to their chemical properties and can be further handled in the mass spectrometer. The fragmented peptides are now in charged droplets separated based on their surface charge, splitting further into smaller droplets until they reach the gas phase as ions. Two physical models describe the process from gas phase to ion called “The ion evaporation model” (IEM) and “The charge residue model” (CRM). 
In the ion evaporation model the droplets shrink by evaporation until ions are expelled \citep{Iribarne1976}. The model had its limitation by explaining same evaporation rate constant among ions with different chemical properties. In the charge residue model the assumption of one molecule per droplet leads to an ionization rate constant, which is independent of the ion itself and relies solely on the generation of the droplet and the efficiency of the solvent \citep{Wilm2011}.

Subsequently after the ionization peptides are accelerated through the magnetic field generated by the mass analyzer. Six general types of mass analyzers are availaible on the market, there are Quadropole, time of flight (TOF), magnetic sector, electrostatic sector, quadropole ion trap and ion cyclotron resonance -mass analyzer. The function of the mass analyzer is to separate ions based on their mass to charge ratio (m/z).

Before the signal of the separated ions can be obtained an optional subsequent step involves trapping the ions an electric and/or magnetic field in order to detect ions based on their (m/z) consecutively. Ion trapping enables the technique of coupled mass spectrometers, where ions were further fragmented by collision induced dissociation and analyzed subsequently. The detector transforms the impact energy of the ion into an electric signal, called spectrum which can be processed computationally.

### Data processing
Given the high resolution of MS data, algorithms are employed to convert the raw signal into an interpretable form. Software packages like MaxQuant \citep{Cox2008} are commonly used to process the data, providing it for further analysis and statistical testing. Other software solutions include Protein Discoverer by Thermo Fisher.

The data obtained from mass spectrometry has three dimensions: m/z ratio, intensity, and retention time. To separate peaks from each other, algorithms are used to identify local minima in the data. The centroid of each peak is determined by fitting a Gaussian peak shape, which can be interpreted as locating the peaks of each m/z spectrum as a function of retention time. The centroid of a peak corresponds to an isotope.

In order to decipher the isotopic distribution of a biomolecule, MaxQuant employs a vertex-based approach. This process, referred to as de-isotoping, utilizes graph theory. It creates a vertex for each individual peak and connects them with their potential isotopic counterparts. This is achieved by determining the mass proportion of an average amino acid to its respective isotope using a concept known as averagine \citep{Senko1995}.  By applying this procedure, the number of data points is reduced by a factor of ten, and each peak represents a small biomolecule.

The subsequent step in data acquisition involves the detection of labels for quantification purposes. Isotopic pairs of the label (such as N13, N14, N15) present in the tag or aminoacid are identified by convolving the two measured isotope patterns with the theoretical isotope patterns. Through an iterative process using a least-squares method, the best fit is determined, enabling the identification of the specific channel or sample.

The intensity-weighted average of the MS peak centroids, as described as 3D peak identification, corresponds to the mass of the peptide. The accuracy of mass measurements relies on the specific analyzer used. For an Orbitrap-type analyzer (which was used during this project), MaxQuant applies a correction value of 1 ppm (parts per million). Autocorrelation between centroids is accounted for by considering only well-identified peptides. Published data indicates that the mass precision within an MS experiment typically ranges around 10^-7.

Each peptide is represented by its individual fingerprint in the MS spectrum, which is based on the sequence and modifications of amino acids. The m/z ratio of the amino acids can be calculated, and the resulting peaks are interpreted as an amino acid sequence. However, only peptide fragments are visible in the spectrum due to peptide fragmentation. To identify proteins, peptides are matched against a sequence database. Sequence databases are typically stored as .fasta files and can be downloaded from the UniProt website (www.uniprot.org). The availability of gene sequences in databases and the computational matching of peptides against those sequences enable the identification of alterations in a sample at the protein level. These alterations can rely on the sequence level or could be to post-translational modifications (PTMs) such as phosphorylation, methylation \citep{Aebersold2003}. The peptide identification (P-) score reflects the goodness of fit between the data and the identified sequence in the database, taking into account the length of the peptide. This score is used to calculate the posterior error probability, which is then utilized to estimate the false discovery rate (FDR). The FDR calculation provides a measure of the rate at which false identifications are expected in the dataset. Within the identified peptide groups, a "razor protein" refers to the protein with the highest number of ambiguously identified peptides. Quantification is performed by considering only unique peptides for comparison. Posterior error probabilities, which represent the likelihood of the identified peptide being a random event, are multiplied together, and only distinct sequences with the highest scores are considered for further analysis. Various metrics that indicate the performance of the peptide search can be utilized in downstream analysis. 

### Downstream analysis
With advancements in mass spectrometry technology and computational methods, proteomic analysis has emerged as a powerful tool for investigating complex protein samples. However, analyzing proteomic data poses challenges in data processing, statistical analysis, and interpretation. This thesis aims to address these challenges by exploring computational methods for downstream analysis of proteomic data. The methods employed in this study will be thoroughly explained in the methods section, with a specific focus on developing an interactive environment equipped with a graphical user interface. This user-friendly interface will enable researchers, including those with limited computational expertise, to navigate and comprehend the data effectively.

The analysis pipeline comprises a series of steps to streamline the data. It begins with quality control measures at the peptide spectrum match level, involving the removal of contaminants and irrelevant channels. Significance levels of peptide spectrum matches serve as confidence thresholds for match selection. The data is then aggregated at the peptide level, followed by additional filtering procedures based on metrics such as reporter ion intensity, peptide covariance to razor proteins, and missing rates for peptides.

To derive meaningful biological insights from the dataset, statistical testing plays a crucial role. Selecting appropriate statistical tests and ensuring proper data transformation and normalization are essential steps. Many statistical methods rely on data distribution and require careful consideration to preserve the biological interpretation of the results. This thesis employs various transformation and normalization techniques, aiming to maintain data integrity and avoid biased interpretations. Additionally, an entropy-based visualization approach will provide users with a valuable tool for validating the computations and selected thresholds for the filtering steps.

Handling missing values poses further challenges in the pipeline development since a missing signal for a peptide results in a missing protein, which is highly unlikely in a biological context. Several options for missing value handling will be included and thoroughly explained in the methods section.

Dimensionality reduction and visualization techniques are widely used to cluster data. In this project, multiple approaches will be utilized, and users will have the ability to visually and interactively observe the data.

It is important to acknowledge that downstream analysis in proteomics lacks a standardized approach, and the selection of computational methods depends on the specific dataset and research objectives. Furthermore, statistical analysis and the interpretation of biological meaning often present misconceptions that need to be addressed. For instance, batch correction, a commonly practiced method, has been overestimated in numerous studies, leading to misleading results \citep{Nygaard2016}. This thesis aims to contribute to the field by providing researchers with a comprehensive understanding of computational methods in proteomics analysis and their application in an interactive environment.








