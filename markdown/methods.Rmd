---
title: ''
output: pdf_document
---
# Materials and Methods

## Materials

For analysis two types of cells were used. One type is the Jurkat-based cell line (J-lat) with integrated HIV. 

The other type of cells are macrophages with a sample size of 72 cells. The analysis is done with two groups. A HIV negative (HIV-) control group and a HIV positive (HIV+) group. 

### Cell Isolation

### Lysis

### Digestion

### Labeling techniques

For differential analysis proteins need to be labeled to compare mass to charge intensities in order to quantify observed peptides. Since mass spectrometry is not a quantitative technique by itself, the peak height or area does not reflect the abundance of a peptide. Physicochemical properties of the proteins can change the ionization efficiency and detectability of the target. However, when comparing the same analyte between multiple runs of labeled proteins, differences in the mass spectrum reflect the abundance of those. Labels should be chosen to change solely the mass of the sample and to not affect folding or other inherent properties of the protein. 

#### Metabolic labeling

Feeding cells with aminoacids containing heavy isotopes, is the method of choice in order to label peptides at the earliest possible level. This atoms can be heavy nitrogen in aminoacids or salts in fertilizer for plants. Mass shifts are proportional to the isotopes incorporated during biomass production and are visible after proteolytic cleavage. Stable isotope labeling in cell culture (SILAC) was presented in the early 2000s. This method used heavy aminoacid enriched media to feed cells, in order to quantitatively analyze expression profiles.    


#### Isobaric labeling

##### Tandem mass tag (TMT)

Tandem mass tag (TMT) reagents enable to differentiate multiple samples analyzing in one MS run. 
The samples are labeled individually and pooled afterwards, this procedure is called multiplexing. TMTs have the same charge and differ only by their isotopic masses, the peaks found for each sample are called reporter ions (RI). Each RI and sample is interpreted as one channel in downstream analysis. The identification of these RI leads to an enrichment and identification of low abundance peptide ions which is common especially in single-cell techniques. With this technique it is possible to quantify proteins and differ low abundant proteins from background noise. 
The disadvantage of isobaric labeling is, that the co-fragmentation signals can be observed in the spectrogram and the data needs to be normalized in order to remove unwanted contribution \citep{Marx2019, Budnik2018}. Furthermore TMTs have an isotopic distribution according to the distribution found in nature. This can be corrected during data-acquisition as a defined spread in other channels.


### Instrumentation

#### Liquid chromatography

In order to separate proteins according to their chemical properties, size or species a liquid chromatography (LC) is recommended before ionization. 

#### Mass Spectrometry

##### Ionization

In order to analyze a biological sample consisting of proteins in solution the liquid needs to be vaporized into gas phase. Two techniques are capable of this procedure. Electrospray ionization (ESI) pushes the analyte through a capillary and applies an electric current to the liquid, vaporizing the sample to a  charged aerosol. Biomolecules are fragmented according to their chemical properties and can be further handled in the mass spectrometer. The fragmented biomolecules are now in charged droplets separated by their charge on the surface, splitting further into smaller droplets until they become a gas phase ion. Two physical models describe the process from gas phase to ion called “The ion evaporation model” (IEM) and “The charge residue model” (CRM). 
In the ion evaporation model (by Iribarne and Thomoson) the droplets shrink by evaporation until ions are expelled. The model had its limitation by explaining same evaporation rate constant among ions with different chemical properties. 
In the charge residue model the assumption of one molecule per droplet leads to an ionization rate constant, which is independent of the ion itself and relies solely on the generation of the droplet and the efficiency of the solvent \citep{Wilm2011}.
 
Matrix-assisted laser desorption/ionization (MALDI)

##### MS.1

##### Coupled mass-spectrometry (MS/MS) & MS.2

In order to enhance sequence identification, two MS devices are built in series. In the first run (MS1) the m/z is determined and the molecules are passed to the next device. Upon passing the molecules are fragmented into smaller ions and analyzed by the second MS. The fragmentation highly depends on the chemical bonds found in the molecule. The majority of these breaks occur on the peptide bond of the protein, although this is not guaranteed for all bonds and so it can happen that certain peptide ions have a low abundance \citep{Budnik2018}. These low abundant peptides will not be detected, hence the problem needs to be faced with another approach . A solution for this problem is molecular barcoding with labeling mentioned in the chapter labeling. 




## Data
```{r Dataflow, eval=FALSE, include=FALSE}
# library(DiagrammeR)
# library(webshot)
# 
# DiagrammeR::grViz("digraph {
# 	graph [layout  =  dot,
# 	       rankdir  = TB]
# 
# node [shape = rectangle]
# rawData [label = 'msData.raw']
# node [shape = oval]
# MQ [label = 'MaxQuant']
# node [shape = invhouse]
# MQ_fd [label = 'feature detection']
# node [shape = cylinder]
# MQ_ds [label = 'Database search']
# node [shape = invhouse]
# MQ_fdr [label = 'FDR calculation']
# MQ_quant [label = 'Protein Quantification']
# node [shape = rectangle]
# MQoutput [label = 'evidence.txt']
# sampleAnnotate [label = 'sample_annotation.txt']
# node [shape = oval]
# ProteomicsWorkbench [label = 'Proteomics Workbench']
# 
# rawData -> MQ -> MQ_fd -> MQ_ds -> MQ_fdr -> MQ_quant -> MQoutput -> ProteomicsWorkbench
# sampleAnnotate -> ProteomicsWorkbench
# }",
# height = 500)
```

### Acquisition

Acquisition of the data was done with MaxQuant \citep{Cox2008} software package. 


#### MS-Spectrum

Each peptide is reflected by its` indivual fingerprint in the ms-spectrum. The fingerprint is based on the chemical properties and modifications of aminoacids. These aminoacids can be calculated through their m/z ration and after that interpreted as an aminoacid sequence. Due to fragmentation of the protein only peptide sequences are visible in the spectrum. In order to identify proteins, peptides are matched against a sequence database \citep{Cox2008}. Sequence Databases are simple .fasta files, which can be downloaded on the uniprot webpage (www.uniprot.org).

Since ms data has a high resolution, algorithms are used to convert the raw signal to an interpretable form. 
MaxQuant  is one of many sofware packages to process the data and provides it for further analysis and statistical testing. Other software solutions are Protein Discoverer Thermo Fisher or even packages for R. In this publication we will mainly focus on the data-acquisition with MaxQuant \citep{Cox2008}. 


#### Three-dimensional peak detection

The three dimensions of the data are: m/z ratio, intensity and retention time. The algorithm finds local minima of the function in order to seperate peaks from each other. The centroid of the peak is detected by fitting a so called gaussian peak shape fitting. This can be interpreted as finding the peaks of each m/z spectrum as a function of time.. The centroid of the peak refers to an isotope.


#### Deisotoping 

To decrypt the istopic distribution of a biomolecule, MaxQuant creates a vertex of every single peak and connects them with their possible isotopic counterparts by finding the proportion of mass of an average aminoacid to its` respective isotope (averagine \citep{Senko1995}). Isotoping is the term of such procedure and it is enabled with graph theory. After this procedure the amount of data points are reduced by a tenfold and a single peak reflects a small biomolecule. 

#### Label detection

The next step in data-acquisition is the detection of labels for quantification. Isotopic pairs of the label (e.g. N13, N14, N15) contained in the tag or aminoacid can be identified by convoluting the two measured isotope patterns with the theoretical isotope patterns. With a least-square method the best fit is found iteratively and the channel/sample can be identified.

#### Improving peptide mass accuracy

The intensity-weighted average of the ms peak centroids (as described in the 3D peak identification) refers to the mass of the peptide. Corrections highly depend on the analyzer, MaxQuant uses for an orbitrap typed analyzer a correction value of 1ppm. Autocorrelation between centroids is compensated by only using well-identified peptides. As published the mass precision within a ms experiment ranges around 10e-7.


#### Peptide search 

Biomolecules can now be searched in a database in forward and reverse direction. The peptide identification (P-) score indicates the fit of the data to the found sequence in the database according to the length of the peptide and is used to calculate the posterior error probability (FDR). The calculation of the false discovery rate is then calculated by taking FDR into contrast. 

#### Protein assembly

After these calculation the identified peptides can be aggregated according to it`s respective protein and  quantified. The mentioned metrics indicating the performance of the peptide search can be used in downstream analysis. A so called razor peptide indicates the group with the highest number of identified peptides. Quantification is enabled by taking only unique peptides into contrast. Posterior error probabilities, which refers to the chance that the found peptide is a random event, are multiplied and only distinct sequences with the highest-scoring are used.

### Data processing

Further analysis is done with R and respective packages such as bioconductor. Since there is no state of the art established, analysis varies upon experimental design. The workflow of the analysis will be processed in a so called pipeline streamlining the data through steps where individual results can be observed in visualizations and indivdual calculations will be adapted according to user demands and experimental properties. 

```{r data_processing_pipeline_flowchart, echo=FALSE, fig.height=17, fig.width=13}
library(Gmisc, quietly = TRUE)
library(glue)
library(htmlTable)
library(grid)
library(magrittr)

data_box_setting <- gpar(fill="#56B4E9")

opts_box_setting <- gpar(fill="lightyellow", col="darkblue")
opts_txt_setting <- gpar(col="darkblue", cex=1)
refinement_arrow_setting <- arrow(angle=30, ends="both", type = "closed")

plot_box_setting <- gpar(fill="#DDCC77")
# input files
evidence_file <- boxGrob("evidence file",
                         box_gp = data_box_setting)
sample_annotation_file <- boxGrob("sample annotation",
                                  box_gp=data_box_setting)

read_data <- boxGrob({glue("read data into q-feature object",
                          .sep = "\n")})


quality_control_1 <- boxGrob({glue("quality control 1",
                          .sep = "\n")})
qc1_content <- boxGrob(
                      "filter out pool samples\nreplace zeros with NA\nfilter out contaminants\nfilter by q-value cutoff",
                       txt_gp = opts_box_setting,
                       box_gp = opts_box_setting,
                       just="center",
                       )

aggregate_psms <-  boxGrob({glue("aggregate peptide spectrum matches to peptides\n
                                 join multiple batches",
                          .sep = "\n")})

quality_control_2 <- boxGrob("quality control 2",
                             box_gp=plot_box_setting)
qc2_content <- boxGrob(
                      "filter by reporter ion intensity cutoff\nfilter by pepitde covariance to razor protein\nremove peptides by missing rate",
                       txt_gp = opts_box_setting,
                       box_gp = opts_box_setting,
                       just="center"
                       )

aggregate_peps <- boxGrob({glue("aggregate peptides to proteins",
                          .sep = "\n")})

transform_data <- boxGrob("transform data",
                          box_gp=plot_box_setting)
transform_opts <- boxGrob(
                      "options:\nlog2\nlog10\nsqrt\nquadratic\nBoxCox\nNone",
                       txt_gp = opts_box_setting,
                       box_gp = opts_box_setting,
                       just="center"
                       )

normalize_data <- boxGrob("normalize data",
                          box_gp=plot_box_setting)
normalize_opts <- boxGrob(
                      "options:\nSCoPE2\nCONSTANd\nNone",
                       txt_gp = opts_box_setting,
                       box_gp = opts_box_setting,
                       just="center"
                       )

missing_value_handling <- boxGrob("missing value handling",
                                  box_gp=plot_box_setting)
missing_value_opts <- boxGrob(
                      "options:\nKNN\ndrop rows\nreplace with mean\nreplace with median\nreplace with 0",
                       txt_gp = opts_box_setting,
                       box_gp = opts_box_setting,
                       just="center"
                       )

batch_correction <- boxGrob("batch correction",
                            box_gp=plot_box_setting)
batch_correction_opts <- boxGrob(
                      "options:\nComBat",
                       txt_gp = opts_box_setting,
                       box_gp = opts_box_setting,
                       just="center"
                       )


dim_red <- boxGrob("dimensionality reduction",
                   box_gp=plot_box_setting)
dim_red_actions <- boxGrob(
                      "procedures:\nPCA\nUMAP",
                       txt_gp = opts_box_setting,
                       box_gp = opts_box_setting,
                       just="center"
                       )

stat_module <- boxGrob({glue("statistics module",
                          .sep = "\n")})

grid.newpage()
vert <- {spreadVertical(files = evidence_file,
                       read_data = read_data,
                       quality_control_1 = quality_control_1,
                       aggregate_psms,
                       quality_control_2 = quality_control_2,
                       aggregate_peps,
                       transform_data = transform_data,
                       normalize_data = normalize_data,
                       missing_value_handling = missing_value_handling,
                       batch_correction = batch_correction,
                       dim_red = dim_red,
                       stat_module
)}
input <- alignVertical(reference = vert$files,
                      evidence_file, sample_annotation_file) %>%
  spreadHorizontal()
vert$files <- NULL


# right side
qc1_content <- moveBox(qc1_content,
                    x = .8,
                    y = coords(vert$quality_control_1)$y)

transform_opts <- moveBox(transform_opts,
                    x = .9,
                    y = coords(vert$transform_data)$y)

missing_value_opts <- moveBox(missing_value_opts,
                    x = .7,
                    y = coords(vert$missing_value_handling)$y)

dim_red_actions <- moveBox(dim_red_actions,
                    x = .9,
                    y = coords(vert$dim_red)$y)

# left side
qc2_content <- moveBox(qc2_content,
                    x = .2,
                    y = coords(vert$quality_control_2)$y)

normalize_opts <- moveBox(normalize_opts,
                    x = .2,
                    y = coords(vert$normalize_data)$y)

batch_correction_opts <- moveBox(batch_correction_opts,
                    x = .2,
                    y = coords(vert$batch_correction)$y)


for (i in 1:(length(vert) - 1)) {
  connectGrob(vert[[i]], vert[[i + 1]], type = "vert") %>%
    print
}

# connect boxes
connectGrob(input[[1]], vert$read_data, type = "N")
connectGrob(input[[2]], vert$read_data, type = "N")

connectGrob(vert$quality_control_1, qc1_content, type = "horizontal", arrow_obj = arrow(angle=0))

connectGrob(vert$quality_control_2, qc2_content, type = "horizontal", arrow_obj = refinement_arrow_setting)

connectGrob(vert$transform_data, transform_opts, type = "horizontal", arrow_obj = refinement_arrow_setting)

connectGrob(vert$normalize_data, normalize_opts, type = "horizontal", arrow_obj = refinement_arrow_setting)

connectGrob(vert$missing_value_handling, missing_value_opts, type = "horizontal", arrow_obj = refinement_arrow_setting)

connectGrob(vert$batch_correction, batch_correction_opts, type = "horizontal", arrow_obj = refinement_arrow_setting)

connectGrob(vert$dim_red, dim_red_actions, type = "horizontal", arrow_obj = arrow(angle=0))

# Print boxes
input
vert
qc1_content
qc2_content
transform_opts
normalize_opts
missing_value_opts
batch_correction_opts
dim_red_actions
```

#### Reading the data

After processing MaxQuant creates a directory containing all results as .txt file. The evidence.txt file include all peptide to spectrum matches (PSM) with their respective proteins and statistical parameters. 

Example fo basic parameters and derivations include:

* Peptide sequence
* Mass to charge ratio (m/z) for all scans (eg. MS1, MS2)
	+ Mass
* Retention time
* Precursor Ion Fragment
	+ source of the detected ion also referred as mother ion
* Fraction of total spectrum 
* Base peak fraction
* Reporter intensity (RI)
	+ corrected RI
* Posterior error probability (PEP)

#### Object oriented programming

In order to streamline the analysis of multiple experiments, object oriented programming can be applied. The approach in R is to create a so called Q-feature object, which contains all variables and metadata in a hierarchical structure. The structure enables sub setting for further analysis \citep{Vanderaa2021}.

#### Zero values
Peptides with low abundance are often set to zero during analysis. However, assigning a value of zero may incorrectly suggest that the sample does not contain the respective peptide. Given that it is highly unlikely for a biological cell of a comparable type and function to not contain a particular protein, replacing the zero value with "not applicable" (NA) is crucial for understanding and interpreting MS data.

#### Exclude reverse matches/contaminants
Peptide sequences matching to the reverse protein sequences (=decoy database) are considered as possible contaminants. These matches can be excluded from further analysis.  

#### Filter according to precursor ion fraction (PIF)
During mass spectrometry, the ions detected in MS1 are further fragmented through collision during multiple MS runs. The resulting product ions are derived from precursor ions (also known as mother ions or parental ions). Contaminant peptides can co-migrate in this process and can be distinguished by the lower fraction of their respective precursor ions \citep{Tannous2020}. These peptides need to be filtered out during the analysis pipeline. A cutoff value, referenced in the SCoPE2 pipeline \citep{Specht2021}, is applied in the user interface, but it can be adjusted according to the needs of the biologist.


#### Filter by q-value
The next step for quality control is the exclusion of samples with a high false discovery rate (FDR). When applying multiple statistical testing (e.g. t-Test) the obtained p-values can be considered as biased, because the probability to observe a significant will iteratively increase with each test performed. Corrections in statistics are an approach to compensate for the multiplicity of testing. There are many ways to do this compensation like the Bonferroni method or Benjamini-Hochberg`s FDR. In Mass spectrometry the common “way to go” is calculating a false discovery rate, by dividing false PSMs (=hit of the decoy database) through the total number of PSMs above the peptide-spectrum matching score. The peptides spectrum matching score is defined as -10log10(p). Whereas the p-value is defined that the hit is done by chance. The calculation of the score is highly dependent on the data acquisition method used. MaxQuant uses Andromeda , an integrated search engine. Proteome Discoverer from Thermo Fisher utilizes different engines such as Mascot or Minora. As published by J.Cox in 2011 Mascot and Andromeda showed similar performance when comparing FDR values as a function of coverage. However the observed performance can be lower when dealing with a decreased coverage \citep{Cox2011}.The threshold for accepting an FDR of an individual PSM is described as q-value. 

#### Peptide spectrum match (PSM) aggregation to peptides 
In data science, aggregation refers to a row-wise operation that merges data based on a particular column using a specific function. In the context of processing from peptides to spectrum matches, the desired column is the peptide sequence. To account for different distributions across multiple assays, the median of the channel is used as the function to aggregate multiple matches into one.

#### Join assays when observing multiple comparable batches at once
Sample size is often a limiting factor in hypothesis testing. A strict quality control and the fact that TMT reagents are only available up to 18-plex can reduce the number of observed samples below the critical threshold, leading to an early end of analyses. To overcome this limitation, the provided software is capable of processing multiple runs simultaneously, allowing for testing of multiple batches and increasing the number of samples that can be included in the analysis.

#### Calculate reporter ion intensity (RI) and filter according to median RI
Columns which do not meet the desired intensity can be filtered by a threshold set on the RI. The median RI can also be used to check if an entire channel has a lower detection level. This can be due two reasons. One is the expression level of the given proteins in a cell. Meaning, that the expression of the observed cell type is simply lower than the other type. Another one could be a spillage of TMT detection in other channels due incorrect or missing correction of the TMT isotopes. 

#### Calculate and filter according to median coefficient of variation (CV) per cell/channel
Depending having a bulk sample or single-cell sample choosing a minimum of observed peptides and a cutoff value for the CV, changes the level of confidence in the peptide data. The coefficient of variation of a peptide is considered as the ratio of the standard deviation to the mean and describes the relationship of the observed peptide signal over multiple proteins (=razor proteins). Peptides having a high coefficient of variation over many razor proteins are considered as noise and need to be filtered out before statistical analysis.


#### Remove peptides with high missing rate
Although missing value imputation can be performed during the analysis of multiple batches, peptides with missing detections across channels can be problematic for quantification. The proteomic composition of a biological sample is similar between replicates and even across groups. However, the threshold of missingness (described as a fraction of the row) can be set in the user interface and adjusted to enable different experimental designs.

#### Aggregation of peptides to proteins 
Similar to the already explained previous aggregation step the peptides will be further processed into their respective proteins after the quality control on the peptide level is performed. Finally an expression matrix for every protein and their respective channel column is returned. Each channel refers to a sample and reflects contains the intensities of each protein found in the biological specimen. 

#### Transformation of protein expression data
Data transformation applies a function to each value of a matrix or array, so that: 
$$
y_i = f(x_i)
$$
Depending on the distribution of the values in the observed expression set, different transformations can be applied to fulfill assumptions for statistical testing. In the shiny application, various procedures are implemented and can be further expanded upon request from the user. The macrophage analysis done by Specht et al. mentioned in their SCoPE2 publication \citep{Specht2021} uses the logarithm to the base 2 to spread a compacted distribution and remove skewness in the dataset. This transformation was used as a reference when comparing methods. However, using the logarithm to the base 10 may be an easily interpretable way of defining expression data and will also be facilitated by other transformation methods such as the boxcox method, which is also implemented in the application.
When observing a wide distribution of small and large values of the expression set in a histogram, a square root transformation can help increase the variability of smaller values and decrease the variability of larger values.
Practically impossible, but in data-driven science, occasionally negative expression values may be present, and they can be converted into positive values by taking each one to the power of two.
Depending on the distribution, the user has to decide which transformation to consider and verify that statistical assumptions are met after application with visualizations such as histograms, qqplots, and MA-plots.
For statistically inexperienced users, the boxcox transformation can help with this decision. Developed in 1964 by Box and Cox \citep{Sakia1992}, this method applies a recursion on the data to determine the statistical parameter lambda. 

Depending on the size of lambda, a certain function will be automatically applied to each value of the expression set in order to introduce normality. In terms of usability, the boxcox method is the most efficient transformation method, taking the decision of which calculation to apply away from the user.


Further filter steps can include correction between multiple runs. This kind of process need the addition of a reference channel. However when observing a single run, these steps are not crucial for the upcoming analysis. 

 


#### Downstream Analysis

##### Principle component analysis (PCA)

Protein levels can be projected to their principle components (PC) and clustered to their specific cell type. So cell types are distinguished by their proteinaceous composition. 

#### Testing for differential expression 

In order to test for differential expression the package limma from R bioconductor was used. \citep{Phipson2016}.  The package uses a linear model approach to define a fold expression between sample groups. Before starting the analysis two matrices need to be computed. The design matrix identifies samples according to the sample type and defines the experimental design. In order to create the matrix automatically a simple algorithm is used within the programmed backend logic. The expression matrix contains intensities for each identified protein will be obtained at the end of the pipeline. A function call with the arguments design and expression matrix calculates the contrast matrix. The next step is creating a linear model between groups by log-ratios of their expression values. 