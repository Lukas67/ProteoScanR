---
title: ''
output: pdf_document
---
# Materials and Methods

## Wetlab - Materials and Methods
For testing and development of the pipeline the following materials and methods were performed. For the sample preparation protocol see appendix. 

### Celltypes
Macrophages from 12 healthy donors and 12 HIV patients with metabolic syndrome and 12 HIV patients without metabolic syndrome. Cell sorting was performed with FACSAria Fusion (BD Biosciences). 

### Digestion
Protein digestion was performed using trypsin (Promega, Madison, WA) buffered in Triethylammonium bicarbonate buffer (TEAB). Reagents were handled with MANTIS automatic dispenser (Formulatrix, Bedford, MA).

### Labeling
Samples of test dataset were labeled with 12-plex TMTpro (Thermo Fisher) in anhydrous acetonitril (ACN) solution.

### Pooling
Carrier proteome was added to the 36 TMTpro sets before pooling sample types for each of the 36 MS-runs.

### Liquid chromatography
In order to separate peptides according to their chemical properties, size or species a liquid chromatography (LC) is recommended before ionization. For the reference data used for developing the pipeline the following intrumentation was used. 
EASY-Spray™ HPLC column, ES802A, 2 $\mu$m × 75 $\mu$m ID × 250 mm (ThermoFisher Scientific)
Acclaim PepMap trap column, 2 $\mu$m × 75 $\mu$m ID × 2 cm (ThermoFisher Scientific)

### Mass Spectrometry

#### Ionization
For the reference data ESI ionization was used.

#### Mass spectrometer
For the reference data used for developing the pipeline an Orbitrap™ Fusion™ Lumos™ tribrid mass spectrometer (ThermoFisher Scientific) was used. The instrumentation uses two subsequent runs in the analyzer (=Orbitrap) to increase mass accuracy. 


## Drylab - Computational methods
```{r Dataflow, eval=FALSE, include=FALSE}
# library(DiagrammeR)
# library(webshot)
# 
# DiagrammeR::grViz("digraph {
# 	graph [layout  =  dot,
# 	       rankdir  = TB]
# 
# node [shape = rectangle]
# rawData [label = 'msData.raw']
# node [shape = oval]
# MQ [label = 'MaxQuant']
# node [shape = invhouse]
# MQ_fd [label = 'feature detection']
# node [shape = cylinder]
# MQ_ds [label = 'Database search']
# node [shape = invhouse]
# MQ_fdr [label = 'FDR calculation']
# MQ_quant [label = 'Protein Quantification']
# node [shape = rectangle]
# MQoutput [label = 'evidence.txt']
# sampleAnnotate [label = 'sample_annotation.txt']
# node [shape = oval]
# ProteomicsWorkbench [label = 'Proteomics Workbench']
# 
# rawData -> MQ -> MQ_fd -> MQ_ds -> MQ_fdr -> MQ_quant -> MQoutput -> ProteomicsWorkbench
# sampleAnnotate -> ProteomicsWorkbench
# }",
# height = 500)
```

### Data-acquisition and raw-file analysis
For the data-acquisition Thermo Fisher acquisition software was used.
Raw-file analysis of the data was done with  MaxQuantCmd 2.2.0.0 \citep{Cox2008}.
The default settings were used. TMTpro labels were corrected according to the isotopical distribution of the Lot-number provided by Thermo Fisher.

```{r data_processing_pipeline_flowchart_vertical, echo=FALSE, fig.height=27, fig.width=16}
library(Gmisc, quietly = TRUE)
library(glue)
library(htmlTable)
library(grid)
library(magrittr)

default_txt_setting <- gpar(cex=1.9)
default_box_setting <- gpar(fill="white")

data_box_setting <- gpar(fill="#56B4E9")

opts_box_setting <- gpar(fill="lightyellow", col="darkblue")
opts_txt_setting <- gpar(col="darkblue", cex=1.5)
refinement_arrow_setting <- arrow(angle=30, ends="both", type = "closed")

plot_box_setting <- gpar(fill="#DDCC77")
# input files
evidence_file <- boxGrob("evidence file",
                         box_gp = data_box_setting,
                         txt_gp = default_txt_setting)
sample_annotation_file <- boxGrob("sample annotation",
                                  box_gp = data_box_setting,
                                  txt_gp = default_txt_setting)

read_data <- boxGrob(glue("read data into q-feature object",.sep = "\n"),
                     box_gp = default_box_setting,
                     txt_gp = default_txt_setting)

quality_control_1 <- boxGrob(glue("quality control 1",.sep = "\n"),
                             box_gp = default_box_setting,
                             txt_gp = default_txt_setting)

qc1_content <- boxGrob("filter out pool samples\nreplace zeros with NA\nfilter out contaminants\nfilter by q-value cutoff",
                      box_gp = opts_box_setting,
                      txt_gp = opts_txt_setting,
                      just="center")

aggregate_psms <-  boxGrob(glue("aggregate peptide spectrum matches to peptides\n
                                 join multiple batches", .sep = "\n"),
                           box_gp = default_box_setting,
                           txt_gp = default_txt_setting)

quality_control_2 <- boxGrob("quality control 2",
                             box_gp = plot_box_setting,
                             txt_gp = default_txt_setting)

qc2_content <- boxGrob("filter by:\nreporter ion intensity cutoff\npepitde covariance to razor proteins\npeptide missing rate",
                       box_gp = opts_box_setting,
                       txt_gp = opts_txt_setting,
                       just="center")

aggregate_peps <- boxGrob(glue("aggregate peptides to proteins", .sep = "\n"),
                          box_gp = default_box_setting,
                          txt_gp = default_txt_setting)

transform_data <- boxGrob("transform data",
                          box_gp = plot_box_setting,
                          txt_gp = default_txt_setting)

transform_opts <- boxGrob("options:\nlog2\nlog10\nsqrt\nquadratic\nBoxCox\nNone",
                          box_gp = opts_box_setting,
                          txt_gp = opts_txt_setting,
                          just="center")

normalize_data <- boxGrob("normalize data",
                          box_gp = plot_box_setting,
                          txt_gp = default_txt_setting)

normalize_opts <- boxGrob("options:\ncol-Median,row_Mean\nCONSTANd\nNone",
                          box_gp = opts_box_setting,
                          txt_gp = opts_txt_setting,
                          just="center")

missing_value_handling <- boxGrob("missing value handling",
                                  box_gp = plot_box_setting,
                                  txt_gp = default_txt_setting)

missing_value_opts <- boxGrob("options:\nKNN\ndrop rows\nreplace with mean\nreplace with median\nreplace with 0",
                              box_gp = opts_box_setting,
                              txt_gp = opts_txt_setting,
                              just="center")

batch_correction <- boxGrob("batch correction & dimensionality reduction",
                            box_gp=plot_box_setting,
                            txt_gp = default_txt_setting)

batch_correction_opts <- boxGrob("ComBat\nPCA\nUMAP",
                                 box_gp = opts_box_setting,
                                 txt_gp = opts_txt_setting,
                                 just="center")

dim_red <- boxGrob("dimensionality reduction",
                   box_gp = plot_box_setting,
                   txt_gp = default_txt_setting)

dim_red_actions <- boxGrob("procedures:\nPCA\nUMAP",
                       box_gp = opts_box_setting,
                       txt_gp = opts_txt_setting,
                       just="center"
                       )

stat_module <- boxGrob(glue("differential expression\npathway enrichment\nontology enrichment",.sep = "\n"),
                       box_gp = default_box_setting,
                       txt_gp = default_txt_setting)

grid.newpage()
vert <- {spreadVertical(files = evidence_file,
                       read_data = read_data,
                       quality_control_1 = quality_control_1,
                       aggregate_psms,
                       quality_control_2 = quality_control_2,
                       aggregate_peps,
                       transform_data = transform_data,
                       normalize_data = normalize_data,
                       missing_value_handling = missing_value_handling,
#                       batch_correction = batch_correction,
#                       dim_red = dim_red,
                       out = stat_module
)}
input <- alignVertical(reference = vert$files,
                      evidence_file, sample_annotation_file) %>%
  spreadHorizontal()
vert$files <- NULL

output <- alignVertical(reference = vert$out,
                      stat_module, batch_correction) %>%
  spreadHorizontal()
vert$out <- NULL


# right side
qc1_content <- moveBox(qc1_content,
                    x = .8,
                    y = coords(vert$quality_control_1)$y)

transform_opts <- moveBox(transform_opts,
                    x = .9,
                    y = coords(vert$transform_data)$y)


# dim_red <- moveBox(dim_red,
#                    x = coords(output[[2]])$x,
#                    y = 0)

batch_correction_opts <- moveBox(batch_correction_opts,
                    x = .8,
                    y = coords(vert$missing_value_handling)$y)

# dim_red_actions <- moveBox(dim_red_actions,
#                     x = (coords(dim_red)$x -.5),
#                     y = coords(dim_red)$x)

# left side
qc2_content <- moveBox(qc2_content,
                    x = .17,
                    y = coords(vert$quality_control_2)$y)

normalize_opts <- moveBox(normalize_opts,
                    x = .2,
                    y = coords(vert$normalize_data)$y)

missing_value_opts <- moveBox(missing_value_opts,
                    x = .2,
                    y = coords(vert$missing_value_handling)$y)


for (i in 1:(length(vert) - 1)) {
  connectGrob(vert[[i]], vert[[i + 1]], type = "vert") %>%
    print
}

# connect boxes
connectGrob(input[[1]], vert$read_data, type = "N")
connectGrob(input[[2]], vert$read_data, type = "N")

connectGrob(vert$quality_control_1, qc1_content, type = "horizontal", arrow_obj = arrow(angle=0))

connectGrob(vert$quality_control_2, qc2_content, type = "horizontal", arrow_obj = refinement_arrow_setting)

connectGrob(vert$transform_data, transform_opts, type = "horizontal", arrow_obj = refinement_arrow_setting)

connectGrob(vert$normalize_data, normalize_opts, type = "horizontal", arrow_obj = refinement_arrow_setting)

connectGrob(vert$missing_value_handling, missing_value_opts, type = "horizontal", arrow_obj = refinement_arrow_setting)

connectGrob(vert$missing_value_handling, output[[1]], type = "N")
connectGrob(vert$missing_value_handling, output[[2]], type = "N")

connectGrob(output[[2]], batch_correction_opts, type = "N", arrow_obj = arrow(angle=0), subelmnt = "r")

# connectGrob(vert$dim_red, dim_red_actions, type = "horizontal", arrow_obj = arrow(angle=0))

# Print boxes
input
vert
qc1_content
qc2_content
transform_opts
normalize_opts
missing_value_opts
batch_correction_opts
output
# dim_red
# dim_red_actions
```

```{r data_processing_pipeline_flowchart_horizontal, eval=FALSE, fig.height=9, fig.width=32, include=FALSE}
library(Gmisc, quietly = TRUE)
library(glue)
library(htmlTable)
library(grid)
library(magrittr)

default_txt_setting <- gpar(cex=1.5)
default_box_setting <- gpar(fill="white")

data_box_setting <- gpar(fill="#56B4E9")

opts_box_setting <- gpar(fill="lightyellow", col="darkblue")
opts_txt_setting <- gpar(col="darkblue", cex=1)
refinement_arrow_setting <- arrow(angle=30, ends="both", type = "closed")

plot_box_setting <- gpar(fill="#DDCC77")
# input files
evidence_file <- boxGrob("evidence file",
                         box_gp = data_box_setting,
                         txt_gp = default_txt_setting)
sample_annotation_file <- boxGrob("sample annotation",
                                  box_gp = data_box_setting,
                                  txt_gp = default_txt_setting)

read_data <- boxGrob(glue("read data into q-feature object",.sep = "\n"),
                     box_gp = default_box_setting,
                     txt_gp = default_txt_setting)

quality_control_1 <- boxGrob(glue("quality control 1",.sep = "\n"),
                             box_gp = default_box_setting,
                             txt_gp = default_txt_setting)

qc1_content <- boxGrob("filter out pool samples\nreplace zeros with NA\nfilter out contaminants\nfilter by q-value cutoff",
                      box_gp = opts_box_setting,
                      txt_gp = opts_txt_setting,
                      just="center")

aggregate_psms <-  boxGrob(glue("aggregate peptide spectrum matches to peptides\n
                                 join multiple batches", .sep = "\n"),
                           box_gp = default_box_setting,
                           txt_gp = default_txt_setting)

quality_control_2 <- boxGrob("quality control 2",
                             box_gp = plot_box_setting,
                             txt_gp = default_txt_setting)

qc2_content <- boxGrob("filter by:\nreporter ion intensity cutoff\npepitde covariance to razor proteins\npeptide missing rate",
                       box_gp = opts_box_setting,
                       txt_gp = opts_txt_setting,
                       just="center")

aggregate_peps <- boxGrob(glue("aggregate peptides to proteins", .sep = "\n"),
                          box_gp = default_box_setting,
                          txt_gp = default_txt_setting)

transform_data <- boxGrob("transform data",
                          box_gp = plot_box_setting,
                          txt_gp = default_txt_setting)

transform_opts <- boxGrob("options:\nlog2\nlog10\nsqrt\nquadratic\nBoxCox\nNone",
                          box_gp = opts_box_setting,
                          txt_gp = opts_txt_setting,
                          just="center")

normalize_data <- boxGrob("normalize data",
                          box_gp = plot_box_setting,
                          txt_gp = default_txt_setting)

normalize_opts <- boxGrob("options:\ncol-Median,row_Mean\nCONSTANd\nNone",
                          box_gp = opts_box_setting,
                          txt_gp = opts_txt_setting,
                          just="center")

missing_value_handling <- boxGrob("missing value handling",
                                  box_gp = plot_box_setting,
                                  txt_gp = default_txt_setting)

missing_value_opts <- boxGrob("options:\nKNN\ndrop rows\nreplace with mean\nreplace with median\nreplace with 0",
                              box_gp = opts_box_setting,
                              txt_gp = opts_txt_setting,
                              just="center")

batch_correction <- boxGrob("batch correction",
                            box_gp=plot_box_setting,
                            txt_gp = default_txt_setting)

batch_correction_opts <- boxGrob("options:\nComBat",
                                 box_gp = opts_box_setting,
                                 txt_gp = opts_txt_setting,
                                 just="center")

dim_red <- boxGrob("dimensionality reduction",
                   box_gp = plot_box_setting,
                   txt_gp = default_txt_setting)

dim_red_actions <- boxGrob("procedures:\nPCA\nUMAP",
                       box_gp = opts_box_setting,
                       txt_gp = opts_txt_setting,
                       just="center"
                       )

stat_module <- boxGrob(glue("statistics module",.sep = "\n"),
                       box_gp = default_box_setting,
                       txt_gp = default_txt_setting)

grid.newpage()

vert <- {spreadHorizontal(files = evidence_file,
                       read_data = read_data,
                       quality_control_1 = quality_control_1,
                       aggregate_psms,
                       quality_control_2 = quality_control_2,
                       aggregate_peps,
                       transform_data = transform_data,
                       normalize_data = normalize_data,
                       missing_value_handling = missing_value_handling,
                       batch_correction = batch_correction,
                       dim_red = dim_red,
                       stat_module
)}

input <- alignHorizontal(reference = vert$files,
                      evidence_file, sample_annotation_file) %>%
  spreadVertical()
vert$files <- NULL


# connect and move the boxes
for (i in 1:(length(vert) - 1)) {
    connectGrob(vert[[i]], vert[[i + 1]], type = "horizontal") %>%
    print}


# right side
qc1_content <- moveBox(qc1_content,
                    x = coords(vert$quality_control_1)$x,
                    y = 0.8)

transform_opts <- moveBox(transform_opts,
                    y = .9,
                    x = coords(vert$transform_data)$x)

missing_value_opts <- moveBox(missing_value_opts,
                    y = .7,
                    x = coords(vert$missing_value_handling)$x)

dim_red_actions <- moveBox(dim_red_actions,
                    y = .9,
                    x = coords(vert$dim_red)$x)

# left side
qc2_content <- moveBox(qc2_content,
                    y = .2,
                    x = coords(vert$quality_control_2)$x)

normalize_opts <- moveBox(normalize_opts,
                    y = .2,
                    x = coords(vert$normalize_data)$x)

batch_correction_opts <- moveBox(batch_correction_opts,
                    y = .2,
                    x = coords(vert$batch_correction)$x)



# connect boxes
connectGrob(input[[1]], vert$read_data, type = "vertical")
connectGrob(input[[2]], vert$read_data, type = "vertical")

connectGrob(vert$quality_control_1, qc1_content, type = "vertical", arrow_obj = arrow(angle=0))

connectGrob(vert$quality_control_2, qc2_content, type = "vertical", arrow_obj = refinement_arrow_setting)

connectGrob(vert$transform_data, transform_opts, type = "vertical", arrow_obj = refinement_arrow_setting)

connectGrob(vert$normalize_data, normalize_opts, type = "vertical", arrow_obj = refinement_arrow_setting)

connectGrob(vert$missing_value_handling, missing_value_opts, type = "vertical", arrow_obj = refinement_arrow_setting)

connectGrob(vert$batch_correction, batch_correction_opts, type = "vertical", arrow_obj = refinement_arrow_setting)

connectGrob(vert$dim_red, dim_red_actions, type = "vertical", arrow_obj = arrow(angle=0))

# Print boxes
input
vert
qc1_content
qc2_content
transform_opts
normalize_opts
missing_value_opts
batch_correction_opts
dim_red_actions
```

### Processing the data
Further analysis is done with R and respective packages such as bioconductor. Please find version info in the appendix.

After processing MaxQuant creates a directory containing all results as .txt file. The evidence.txt file includes all peptide to spectrum matches (PSM) with their respective proteins and statistical parameters. 

Example fo basic parameters and derivations include:

* Peptide sequence
* Mass to charge ratio (m/z) for all scans (eg. MS1, MS2)
* Retention time
* Precursor Ion Fragment
* Fraction of total spectrum 
* Base peak fraction
* Reporter intensity (RI)
* Posterior error probability (PEP)

### Object oriented programming

In order to streamline the analysis of multiple experiments, object oriented programming was applied. The approach in R is to create a so called Q-feature object, which contains all variables and metadata in a hierarchical structure. The structure enables sub setting for further analysis \citep{Vanderaa2021}.

### Zero values
Peptides with low abundance are often set to zero during analysis. However, assigning a value of zero may incorrectly suggest that the sample does not contain the respective peptide. Given that it is highly unlikely for a biological cell of a comparable type and function to not contain a particular protein, replacing the zero value with "not applicable" (NA) is crucial for understanding and interpreting MS data. NA values will be managed in missing value handling later on the protein level.

### Exclude reverse matches/contaminants
Peptide sequences matching to the reverse protein sequences (=decoy database) are considered as possible contaminants. These matches will be excluded from further analysis.  

### Filter according to precursor ion fraction (PIF)
During mass spectrometry, the ions detected in MS1 are further fragmented through collision during multiple MS runs. The resulting product ions are derived from precursor ions (also known as mother ions or parental ions). Contaminant peptides can co-migrate in this process and were distinguished by the lower fraction of their respective precursor ions \citep{Tannous2020}. These peptides need to be filtered out during the analysis pipeline. A cutoff value, referenced in the SCoPE2 pipeline \citep{Specht2021}, is applied in the user interface, but it can be adjusted.

### Filter by q-value
The next step for quality control is the exclusion of samples with a high false discovery rate (FDR). When applying multiple statistical testing (e.g. t-Test) the obtained p-values can be considered as biased, because the probability to observe a significant will iteratively increase with each test performed. Corrections in statistics are an approach to compensate for the multiplicity of testing. There are many ways to do this compensation like the Bonferroni method or Benjamini-Hochberg`s FDR. In Mass spectrometry the common “way to go” is calculating a false discovery rate, by dividing false PSMs (=hit of the decoy database) through the total number of PSMs above the peptide-spectrum matching score. The peptides spectrum matching score is defined as $-10 * log(p)$. Whereas the p-value is defined that the hit is done by chance. The calculation of the score is highly dependent on the data acquisition method used. MaxQuant uses Andromeda, an integrated search engine. Proteome Discoverer from Thermo Fisher utilizes different engines such as Mascot or Minora. As published by J.Cox in 2011 Mascot and Andromeda showed similar performance when comparing FDR values as a function of coverage. However the observed performance can be lower when dealing with a decreased coverage \citep{Cox2011}.The threshold for accepting an FDR of an individual PSM is described as q-value. 

### Peptide spectrum match (PSM) aggregation to peptides 
In data science, aggregation refers to a row-wise operation that merges data based on a particular column using a specific function. In the context of processing from PSMs to unique peptides, the desired column is the PSM sequence. To account for different distributions across multiple assays, the median of the channel is used as the function to aggregate multiple matches into one.

### Join assays when observing multiple comparable batches at once
Sample size is often a limiting factor in hypothesis testing. A strict quality control and the fact that TMT reagents are only available up to 18-plex can reduce the number of observed samples below the critical threshold, leading to an early end of analyses. To overcome this limitation, the provided software is capable of processing multiple runs simultaneously, allowing for testing of multiple batches and increasing the number of samples that can be included in the analysis. For further processing the runs are joined in one object at the peptide level.

### Calculate reporter ion intensity (RI) and filter according to median RI
Columns which do not meet the desired intensity can be filtered by a threshold set on the RI. The median RI can also be used to check if an entire channel has a lower detection level. This can be due two reasons. One is the expression level of the given proteins in a cell. Meaning, that the expression of the observed cell type is simply lower than the other type. Another one could be a spillage of TMT detection in other channels due incorrect or missing correction of the TMT isotopes. The correction values can be found on the vendor`s webpage (in this case Thermo Fisher) and correction needs to be set up in MaxQuant.

### Calculate and filter according to median coefficient of variation (CV) per cell/channel
Depending having a bulk sample or single-cell sample choosing a minimum of observed peptides and a cutoff value for the CV, changes the level of confidence in the peptide data. The coefficient of variation of a peptide is considered as the ratio of the standard deviation to the mean and describes the relationship of the observed peptide signal over multiple proteins (=razor proteins). Peptides having a high coefficient of variation over many razor proteins are considered as noise and need to be filtered out before statistical analysis.

### Remove peptides with high missing rate
Although missing value imputation can be performed during the analysis of multiple batches, peptides with missing detections across channels can be problematic for quantification. The proteomic composition of a biological sample is similar between replicates and even across groups. However, the threshold of missingness (described as a fraction of the row) can be set in the user interface and adjusted.

### Aggregation of peptides to proteins 
Similar to the already explained previous aggregation step the peptides will be further processed into their respective proteins after the quality control on the peptide level is performed. Finally an expression matrix for every protein and their respective channel column is returned. Each channel refers to a sample and contains the intensities of each protein found in the biological specimen. 

### Transformation of protein expression data
Data transformation applies a function to each value of a matrix or array, so that: 
$$
y_i = f(x_i)
$$
Depending on the distribution of the values in the observed expression set, different transformations can be applied to fulfill assumptions for statistical testing. In the shiny application, various procedures are implemented and can be further expanded upon request from the user. The macrophage analysis done by Specht et al. mentioned in their SCoPE2 publication \citep{Specht2021} uses the logarithm to the base 2 to spread a compacted distribution and remove skewness in the dataset. This transformation was used as a reference when comparing methods. However, using the logarithm to the base 10 may be an easily interpretable way of defining expression data and will also be facilitated by other transformation methods such as the boxcox method, which is also implemented in the application.
When observing a wide distribution of small and large values of the expression set in a histogram, a square root transformation can help increase the variability of smaller values and decrease the variability of larger values.
Depending on the distribution, the user has to decide which transformation to consider and verify that statistical assumptions are met after application with visualizations such as histograms, qq-plots, and MA-plots.
For statistically inexperienced users, the boxcox transformation can help with this decision. Developed in 1964 by Box and Cox \citep{Sakia1992}, this method applies a linear model against 1 on the data to determine the statistical parameter lambda by the maximum likelihood method. The log-likelihood method takes the 95% confidence interval for the parameter lambda and the final lambda is chosen as the value with the highest log-likelihood value. 

\begin{center}
\begin{tabular}{l r}
$\lambda$ & Transformation \\
-2 & $\frac{1}{x^2}$ \\
-1 & $\frac{1}{x}$ \\
-0.5 & $\frac{1}{\sqrt{x}}$ \\
0 & $\log{x}$ \\
0.5 & $\sqrt{x}$ \\
1 & $x$ \\
2 & $x^2$
\end{tabular}
\end{center}

Depending on the size of lambda, a certain function will be automatically applied to each value of the expression set in order to introduce normality. In terms of usability, the boxcox method is the most efficient transformation method, taking the decision of which calculation to apply away from the user.

### Normalization
The term normalization is ambiguous in data-science and will be explained briefly in this chapter. Starting with converting the data to the Z-distribution (Gaussian result), where 0 represents the mean and 1 represents the standard deviation, in vectorization every feature (or protein) is represented as a vector that points in a specific direction in the unit sphere, normalization can have different meanings. Normalization is considered as part of the scaling procedure and ensures that every feature contributes equally to the statistical model and hinders large values from biasing the model in a particular direction. However, this could also have negative consequences for the modeling procedure, as it may reduce the impact of important features on the dataset. 
Depending on the algorithms later used in the data processing, the method of choice for normalization can change the results significantly. The K-means clustering algorithm, for instance relies on the distances between distinct data points by minimizing variance of the squared euclidean distances. This method will be applied in the missing value imputation, which will be the next step applied to the data. 
The first normalization method available in the user interface is column-wise median and row-wise mean normalization. This method was chosen by Specht et al in the SCoPE2 publication 2021 \citep{Specht2021} and worked as a reference during the development process to benchmark other methods. When dividing each value of an expression set by the median of the particular column, the procedure is considered as a column-wise median normalization. Although the mean-normalization works the same way but takes the mean as the second variable. In the analysis pipeline both procedures were applied on the dataset. However when the logarithm as the transformation method was chosen as transformation method, the pipeline automatically switches from a division to a subtraction, since:
$\log_a \left(\frac{u}{v}\right)=\log_a u-\log_a v$. After applying this normalization method, each value can be interpreted as a distance or fraction of the mean or median of the corresponding column or row, enabling fair comparisons between features and samples.
The second method for normalization included is the CONSTANd method, which was proven suitable for relative quantification in the field of proteomics \citep{Maes2016, VanHoutven2021}, but can also be applied on RNAseq data. CONSTANd uses a technique called matrix raking, which employs the RAS algorithm. The expression set reflects the non-negative real $(m, n)$ matrix $A$ were the bi proportional constrained matrix problem will be solved by finding the $(m, n)$ matrix $B$ which equals to $diag(x) * A *diag(y)$ . Whereas $x \in \mathbb{R}^{m}$ and $y \in \mathbb{R}^{n}$. The solution to this problem involves finding the row sum of $B=u_i$ and the column sum of $B=v_j$, where $i$ and $j$ denote the row and column indices of the matrix, respectively \citep{Bacharach1965}. In other words, the matrix will be alternatively manipulated on rows and columns until the mean of both equals 1. In order to apply the method and yield true results, one has to consider 3 major assumptions between sample types. This assumption can be observed in the MA-plot which indicates the differences of two samples. Given the intensities of two samples, $R$ and $G$, visualized on a graph with the x-axis as $M=\log_2(\frac{R}{G})$ and the y-axis as $A=\frac{1}{2}\log_2(RG)$. A comparison between all sample types of identically processed sets is needed before calculation and the assumptions must be fulfilled.

\begin{itemize}
\item{\textbf{1}} \textbf{The majority of proteins are not differentially expressed}
\\Density of the dots decreases when looking from the middle of the cloud towards the edges.

\item{\textbf{2}} \textbf{Up- and downregulation is balanced around the mean expression}
\\The scatterplot of the data points is symmetrical around the mean on the horizontal axis. 

\item{\textbf{3}} \textbf{Systematic bias correlates with the magnitude of expression}
\\The symmetry axis is approximately horizontal.

\end{itemize}

If the MA-plot does not visualize any of these violations the CONSTANd method can be applied on the dataset and the biological meaning of the sample types can be assessed. The method is utilized by the bioconductor package handler. 
Another method developed in the application is the quantile normalization. Quantile normalization is performed over all columns, equalizing statistical parameters such as inter quartile range, median and mean. However the application of this method is only suitable if the number of differentially expressed proteins is approximately equal over sample types. Otherwise statistical power could be lost by removing fine deviations \citep{Zhao2020}. For the quantile normalization method the package limma was utilized. Quantile normalization ranks protein expression by magnitude, then averages the expression values occupying the same rank and substitute the inital value with the average.
However if the dataset already consists of balanced features and samples or the user wants to benchmark the methods applied, normalization can be skipped also, by setting to "none".

### Missing value handling
When analyzing multiple batches simultaneously, it is possible to encounter situations where a particular protein is not detected in one of the batches. From a biological perspective, it is highly unlikely that a cell completely lacks a single protein, as it would imply a complete loss of function for that protein. Therefore, an intensity value of 0 is close to impossible for most proteins. In order to handle this dark space in the expression set, several methods can be chosen in the application. 
A variety of function employs replacing the missing values with the mean or median of the rest of the matrix. This procedure introduces no bias in the data if the missingness is low, although differential expression for the sample carrying the missing feature can not be expected as well. Even a more conservative approach is dropping the rows for the feature with the missing value in one of the observed samples. 
A common practice in proteomics is utilizing the K-nearest neighbor algorithm (=KNN), which imputes the missing values by a euclidean metrics of the neighboring values in the columns where the feature is not missing. $K$ denotes for the count of neighboring values of a gene and can be selected upon preference. The euclidean distance between two points is defined as $d = \sqrt{a^2 * b^2}$, the $K$ nearest neighbors average value will be assigned to the intensity of the missing feature. Since the average is a statistical metric which can be biased easily by skewed distributions, normalization is an important task before performing missing value imputation. KNN was benchmarked  for the regression of protein abundance intensities and showed an Area under the Curve (AUC) above 0.7 when testing on a human proteome test set \citep{Lan2013}.

### Batch correction 
Since mass spectrometry experiments are highly influenced by several factors, such as sample processing, the technician, manufacturer and production of the device, the reagents etc., working in multiple batches introduces further variance in the protein data. This variance would overshadow the biological differences across sample types when performing dimensionality reduction. A common misconception in biology is, that batches need to be corrected before differential expression analysis, which is the final aim of the developed pipeline. It appears convenient to remove the batch effect before performing any statistical analysis, which is in turn prone to errors especially if the sample groups are not split equally between batches. In this case, group differences influence the batch effect reducing statistical power after the correction. These both effects are so called interdependent. The opposite could happen if the experimental design is heavily unbalanced leading to group differences induced by batch correction. 
Finally it is recommended to include the batch factor should in the linear model as a co-factor and statistically quantify it \citep{Nygaard2016}.
However it could be of interest to observe biological relevance of the factor of interest in the dimensionality reduction visualization, so a batch correction option was included to meet this need.
To address the non-biological experimental variations, also known as technical factors, the R package sva \citep{Leek2012} with the ComBat function \citep{Johnson2007} was used. 
The expression of a gene $g$ for the batch $m_i$ and sample $j$ can be defined as  $Y_{ijg} = a_{g} + X\beta_{g} + \gamma_{ig} + \delta_{ig}\epsilon_{ijg}$, where $\gamma$ and $\delta\epsilon$ represent the systematic error caused by the batch. 
The Bayesian framework is employed to remove this systematic error by shrinking the effect and pooling information over the features $g$. Ideally the effects $a$ and $\beta$ remain as natural as they are and should reflect the true biological difference between experimental obtained samples or groups. Before applying batch correction, the data is standardized using Z-transformation. By the method of moments (= the mean and the variance) the two batch effect parameters $\gamma$ and $\delta\epsilon$ were estimated and can be subtracted from each value for the particular feature driving statistical moments at a comparable level between batches.
The ComBat function allows the user to specify a desired effect, which represents the actual biological variance to be preserved while removing non-biological variance. This is achieved by constructing a design matrix, which is provided as prior information about the dataset within the Bayesian framework. However, in cases of poor experimental design, the factors contributing to unwanted and desired variation may be confounded. In the worst case, if only one sample type per run is observed, it becomes challenging for the program to distinguish between batch-induced variance and biological variance. In such cases, the user is advised to adjust the experimental layout for future experiments. 
The detection algorithm for confounding effects is facilitated by QR matrix decomposition. The matrix $A=QR$ is decomposed into an orthogonal matrix $Q$ and an upper triangular matrix $R$. 
By comparing the rank of matrix $A$ with the number of biologically relevant factors, it is possible to determine the presence of confounding effects in the dataset. The rank is defined as the maximum number of linearly independent columns. The same approach is used in the limma package \citep{Phipson2016} to identify non-solvable coefficients in experimental designs.

### Dimensionality reduction
After cleaning the data and crystallizing from impurities such as noise and unwanted effects, the biological differences between sample types can be obtained. Proteomic data consists of multi dimensions (=features) which are overall hard to understand by a glance. An approach to observe the differences without a closer sight of exact effect on the proteinaceous ensemble, but still keep a high level of information can be dimensionality reduction. After the cleaning procedures any additional effect caused by technical or biological replication should be removed, however it could be found be highlighting according to the particular factor. This makes the dimensionality reduction a useful tool before starting any statistical analysis by checking for involuntarily introduced bias. 

#### Principle component analysis (PCA)
The most common approach in data science for any kind of high dimensional data is the PCA. For the pipeline the bioconductor package scater \citep{McCarthy2017} was used. Every principle component can be understood as a vector pointing in a specific direction derived as the eigenvector from the covariance matrix of the expression set. The covariance matrix indicates for the co linearity between the variance of all elements compared. After calculating the covariance matrix the eigenvectors for the covariance matrix will be obtained by solving the quadratic equations to obtain the eigenvalues $\lambda$. Since $A v = \lambda v$ the eigenvector $v$ can be obtained by solving the equation for each element of the vector. The eigenvectors $v$ never changes is direction and explains the covariance within the dataset and so also the mathematical differences between the samples. The eigenvectors are sorted according to their eigenvalues in decreasing order and therefore decreasing explained variance. For graphical visualizations the principle components are plotted against each other. The first two components, which explain the majority of variance in the dateset highlight the difference between samples. 

#### Uniform manifold approximation & projection (UMAP)
UMAP \citep{McInnes2018} is a dimensionality reduction technique that was developed in recent years and is based on Riemann geometry. It shares similarities to the  t-distributed stochastic neighbor embedding (t-SNE) in terms of graphical visualization. Riemann geometry is a mathematical framework that deals with three dimensional functions using volumes, areas and vectors, which are also concept of topological data analysis. 
Before performing UMAP, three assumptions need to be considered:

\begin{enumerate}
\item There exists a manifold on which the data would be uniformly distributed.
$\rightarrow$ This assumption implies that the probability for each value in the biological dataset is the same.

\item The underlying manifold of interest is locally connected.
$\rightarrow$ In other words, similar samples will have a similar proteinaceous ensemble.

\item Preserving the topological structure of this manifold is the primary
goal.
$\rightarrow$ This means that the homeomorphic structure (the property of being able to transform one shape into another without tearing or gluing) is locally preserved, and local values in the dataset can be compared with each other.
\end{enumerate}
\citep{McInnes2018}

The UMAP algorithm begins by constructing a graph with a defined neighborhood parameter, typically denoted as $k$, which can be selected computationally. UMAP employs the k-nearest neighbor descent algorithm (NN-descent). Similar to the k-nearest neighbor algorithm (KNN), used in the missing value imputation part of this project, NN-descent recursively iterates to find the smallest distance to the respective neighbors within the defined neighborhood size $k$. Increasing the value of $k$ gradually strengthens the clustering of the data, but at a certain point, fine structure may be lost, and only rough estimates of the underlying principle can be observed.
The mathematical graph in UMAP is defined by nodes connected by edges, with each node internally connected to at least one other node. Edges are distinguished from each other not only by their connection but also by their importance, which is often referred to as weight. Whereas the weight function is calculated as 

\[w((x_i, x_{i_{j}})) = \exp(\frac{-max(0,d(x_{i},x_{i_{j}}-\rho_{i})}{\sigma_{i}})\] 

and determine the parameter $\rho$ for the weight function that: 

\[\rho_i=\min \left\{ d(x_i,x_{i_{j}}) \,|\, 1 \leq j \leq k, d(x_i, x_{i_{j}}) > 0 \right\}\]

Furthermore $\sigma$ will be set to meet the following conditions: 
\[\sum_{j=1}^{k} \exp ( \frac{ -\max(0, d(x_{i}, x_{i_{j}}) - \rho_i)} { \sigma_i } ) = \log_2(k)\] 
and acts as a normalization factor. The obtained graph $\overline{G}$ is called the fuzzy simplicial set which is obtained by approximating the geodesic distance (= locally length-minimizing curve) of the data points, leading to a topological approximation. 
The second phase of the algorithm adjusts the layout of the weighted graph to a representative view, but preserves the characteristics and shows the underlying topological principle of the data. In order to obtain a visual representation of the graph, the algorithm applies repulsive forces:
\[\frac{2b}{(\epsilon + ||y_i-y_j||_{2}^{2})(1+a||y_i-y_j||_{2}^{2b})}(1-w((x_i,x_j)))(y_i-y_j)\]
among vertices and attractive forces
\[\frac{ -2ab || y_i -y_j||_{2}^{2(b-1)} }{1+||y_i-y_j||_{2}^{2}} w((x_i,x_j))(y_i-y_j)\] 
along edges. Where $a$ and $b$ are denoted as hyperparameters. By iterating through possible conformations until converging towards a local minimum with decreasing repulsive and attractive forces, the UMAP reaches it`s aim to reveal the underlying properties of the data. The application utilizes the bioconductor package scater for performing UMAP \citep{McCarthy2017}.

### Validation of the methods applied to the dataset
For the validation of methods applied to the dataset, the mutual information calculation is used to assess the impact of computations and the information it carries. 
Mutual information, denoted as $I(X;Y)$, is a measure of the dependence or information shared between two variables, $X$ and $Y$, in this case sample types. It is calculated using the equation $I(X;Y) = H(X,Y) - H(X|Y) - H(Y|X)$, where $H$ represents the entropy, $H(X|Y)$ and $H(Y|X)$ denote for the both conditional probabilities and $H(X,Y)$ for the joint entropy. 
Entropy, as a concept in information theory, quantifies the level of disorder or uncertainty associated with a random event. Events that are highly likely to occur are considered less informative than events that are more unexpected or less likely. In a Venn diagram $I(X;Y)$ (the mutual information) corresponds to the intersection of the two conditional entropys $H(X|Y)$ and $H(Y|X)$. To calculate the mutual information between two variables $X$ and $Y$ the above equation can be alternatively expressed as $I(X;Y) = D_{KL} (P_{(X,Y)} || P_{X} \otimes P_{Y})$. $D_{KL}$ denotes for the Kullback-Leibler \citep{Kullback1951} divergence which is a concept of relative entropy between two variables and tells us how much 2 probability distributions differ from each other. $P_{X} \otimes P_{Y}$ represents the product of the marginals. The underlying principle of divergence is the integral asymmetry in Bayesian inference, which means that divergence does not satisfy the triangle inequality. Differences are not the same depending on the starting point. By comparing the mutual information before performing any calculations with the mutual information calculated after the computations, it is possible to assess whether the applied methods have significantly altered the dependence or information content. The goal is to choose a method which preserves the information contributed by each biological sample type to the experiment.
The R package "infotheo" is utilized to perform the estimates for the entropy of the two variables. Mutual information is returned as natural unit of information (=nat), a measurement proportional to the to the Shannon entropy ($1nat = \frac{1}{ln2} shannons$). Before calculating the pairwise mutual information of the expression matrix, values were discretized into bins with the discretize function of above package.
The mutual information is calculated within each sample type, and it can also be calculated between a selection of sample types to determine if the computations performed affect the information content. After that all pairwise mutual information within the expression set before and after the pipeline is obtained. The pairwise differences can be observed as a boxplot for a particular sample type indicating the change in dependence within and also compared to another sample type.  

### Statistics
#### Differential expression
After visualizing the biological tendency in the dataset, the next step is hypothesis testing. Several experimental designs can be tested against the null hypothesis $h_0$, which states that there is no significant difference between the sample types. Before proceeding with any statistical testing, it is important to observe the data using either a quantile-quantile plot (qq-plot) or a histogram. The qq-plot serves a vital role in exploratory statistics by visualizing whether two datasets follow the same underlying distributions. In the case of statistical preparation for parametric tests, the first distribution is the dataset of interest, while the second dataset is the normal distribution. If the dataset follows an exact normal distribution, the values will align with the red line in the chart. However, values at the ends of the distribution may deviate from the normality line, indicating differentially expressed genes. These genes can be observed both in the histogram and the qq-plot. 
Parametric statistical tests rely on the assumption of normality in the data, which needs to be achieved before building a model. To achieve normality, it is recommended to try various combinations of transformation and normalization methods. By observing the results in the plots, one can determine which combination of data processing methods satisfies the conditions required for the tests. However the limma method utilized in the statistics module shows great robustness to imperfect normal distributed values \citep{Ritchie2015}.

To test for differential expression, the analysis utilizes the "limma" package from R Bioconductor \citep{Phipson2016}. This package employs a linear model approach to determine the fold expression between sample groups. Before initiating the analysis, two matrices need to be computed. The design matrix identifies samples based on their sample type and defines the experimental design. An algorithm within the program's backend logic is utilized to automatically generate this matrix according to the user's selection in the interface. The expression matrix contains intensities for each identified protein and is obtained at the end of the pipeline. By calling a function with the design and expression matrices as arguments, the contrast matrix is calculated, enabling the user to decide which comparisons to consider and also perform tests on multiple factors. When observing multiple batches at once, as already explained in the chapter batch correction this effect needs to be taken into consideration when performing differential expression analysis. This is done by the multi-factor option in the user interface by selecting the batch as a co-factor and inclusion in the design matrix. The advantage of this procedure is, that the biological relevance of working in multiple batches can be quantified and the publishing of false positive results avoided.
The multi factor option uses an additive model as a design matrix, whereas continuous variables (such as size, weight etc.) can also be used to build the statistical model. 

The subsequent step involves creating a linear model between groups using log-ratios of their expression values. In a two-way design, the expression of the gene $y$ can be explained as $Exp_{Y} = \beta_{0} + \beta_{1}X_{1} + \epsilon$, where $\beta_0$ represents the intercept, which can be interpreted as the mean expression of the gene. $\beta_1$ represents difference in mean of the treatment (or condition) on the discrete or continuous variable $X_{1}$, and $\epsilon$ acts as an error term. The interface allows for additive models with multiple factors, allowing the user to add factors according to various scenarios. For example, in a two-factor model, it would look like: $Exp_{Y} = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \epsilon$. The principle remains the same for models with multiple factors. When visualizing the linear model, the x-axis would represent the gene for the sample types or other factors, whereas the y-axis would account for the expression value. By drawing a line through the cloud of data points the model is constructed by minimizing the quadratic distance to every data-point, called the least-square method or also ordinary least-square method. With the regression line the above mentioned equation can be constructed. 

Next the statistical module of the program performs an empirical Bayes model utilizing the eBayes \citep{Smyth2004} function of the Limma package. The eBayes function utilizes a moderated t-statistics following t-distributed values. The advantage of this method over the posterior odds is the reduction of hyperparameters to estimate used for the modelling process, also called shrinkage. 
In this context, the hyperparameters to estimate are the coefficient $\beta_{gj}$ for gene $g$ and sample $i$, as well as the variance $\sigma^2_g$ across genes $g$. In the Bayesian approach, probabilities are updated after obtaining new data, referred to as conditional probabilities. For estimation, it is assumed that $\frac{1}{\sigma^2_g} \sim \frac{1}{d_{0}s^2_{0}} \chi^2_{d_{0}}$, representing the prior information of the model.
The probability for $\beta_{gj} \neq 0$ is denoted as $p_j$, where $p_j$ represents the expected proportion of truly differentially expressed genes. The expected distribution of log-fold changes follows the distribution $\beta_{gj} | \sigma^2_g , \beta_{gj} \neq 0 \sim N(0, v_{0j}\sigma^2_g)$.
The posterior mean of $\sigma^2_g$ given $s^2_g$ is calculated as $s^{-2}_g = E(\sigma^2_g | s^2_g) = \frac{d{0}s^2_{0} + d_{g}s^2_{g}}{d_{0} + d_{g}}$. This is where the shrinkage occurs, as the posteriors affect the prior values based on their respective sizes and degrees of freedom.
The moderated t-statistic is defined as $\tilde{t}_{gj} = \frac{\hat{\beta}_{gj}}{\hat{s}_{g}\sqrt{v_{gj}}}$. Once the values of $\tilde{t}$ and $s^{2}$ are calculated, the posterior odds are computed, providing the odds that a particular gene is differentially expressed. The odds for gene $g$ being differentially expressed are denoted as $O_{gj} = \frac{p(\beta_{gj} \neq 0 | \tilde{t}_{gj}, s^2{g})}{p(\beta_{gj} = 0 | \tilde{t}_{gj}, s^2{g})}$, where the numerator represents the probability that the gene is differentially expressed, and the denominator represents the probability that the gene is not differentially expressed \citep{Smyth2004}.

Limma outputs a table with all proteins, their respective logfold change, p-values and adjusted p-values. The user can choose different methods for the correction of the p-values, which is necessary when testing the same hypothesis multiple times. A common method in omics studies is the false discovery rate (=fdr) by Benjamini Hochberg \citep{Benjamini1995}. By ranking all the p-values from smallest to largest the p-values are adjusted sequentially using the formula $p_(i) = \frac{r_i}{m} Q$. The rank in the p-value table is denoted as $r_i$   , $m$ reflects the total number of tests and $Q$ represents false discovery rate which is set to 5%. The second option to choose is the Benjamini & Yekutieli (=BY) \citep{Benjamini2001}method, which is a further development of the fdr by the same statistician and has a wider range of application based on the dependencies for the test.
The third and most conservative option is the Holm method \citep{Holm1979} which controls the error sequentally in a family wise manner, which is considered as traditional in regards to the above mentioned experimental wise error rates. The above mentioned correction methods are ordered in the way from the most liberal to the most conservative one. The selection is supposed to give the user the possibility questioning results on multiple levels. 

### Protein set enrichment analysis
After differential expression the upcoming question could be, in which context the up or down regulated proteins are. Enriched proteins can be understood as an over-representation (ORA = over-representation analysis) within a specific set. This set can be a pathway or a selection of common denominators in biological contexts, such as cell types or oncological factors. The statistical testing to identify the overabundant proteins of a specific set is similar to a $\chi^2$ test developed by Karl Pearson \citep{Pearson1900} or the exact Fisher test \citep{Sprent2011} for tables with 4 fields in small sample sizes. Within the list are all truly differentially expressed genes, an annotation is the identification of a protein in the desired biological context. 
The table is expected to be populated by the values as follows:

\begin{center}
\begin{tabular}{|c|| c| c|| c|} 
 \hline
   & in list & not in list & totals \\ [0.5ex] 
 \hline\hline
 with annotation & (A+B)(A+C)/N  & (A+B)(B+D)/N & A+B \\ 
 \hline
 without annotation & (C+D)(A+C)/N & (C+D)(B+D)/N & C+D \\
 \hline
  & A+C & B+D & N \\
 \hline
\end{tabular}
\end{center} 

Under the null hypothesis $H_{0}$ states that the distribution and population of the table is random and there is no clear tendency with significant evidence. Against that the alternative hypothesis $H_{A}$ says that there is an underlying tendency the table is populated. 
This leads to the test statistics, the observed values (=$O$) in the table are significantly different to the expected (=$E=\frac{(A+B \lor C+D)}{N}$ ones. One approach is approximating $\chi^2= \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i}$ and deriving the p-value by the area under the density curve of the $\chi^2$ distribution $Q = \sum_{i=1}^{k} Z_{i}^{2}$. In the distribution $k$ is defined by the degrees of freedom (=df), what can be obtained by $k=(columns-1)*(rows-1)=df$ of the table. Another approach to obtain an exact p-value would be Fisher`s exact test, which states that the margins are distributed according to the hypergeometric distributions. The test statistic leads directly to the p-value with no need for approximation: 
$p=1-\sum_{i=0}^{A-1}\frac{\binom{B}{i}\binom{D-B}{A+C-i}}{\binom{D}{A+C}}$
Here D denotes either for the background distribution which can be the total number of proteins obtained in the experiment, all proteins with annotation or a custom selected background. The background indicates all proteins which could be positive tested for in the observed sample. The above mentioned three options are implemented in the user interface.
After obtaining the p-values for each protein set, the subsequent critical step involves the correction for multiple testing, considering the inherent risk of false positive results. Multiple testing correction methods adjust the significance thresholds to account for the increased probability of detecting false positives when performing multiple statistical tests. In this protein set enrichment analysis, the user is provided with a selection of established correction methods, akin to those commonly utilized in the field of differential expression analysis, ensuring robust and reliable interpretation of the results. 
Once the enriched protein pathways are determined, they can be visually represented using bar or dotplots, and a network of the found pathways. In the network ontologies are depicted as nodes, whereas edges show the strength of the interaction by thickness corresponding to the number of associated proteins. 

#### Pathway based protein enrichment analysis
The pathway-based protein enrichment analysis is performed using the R Bioconductor package clusterProfiler \citep{Wu2021}. The analysis utilizes the “Kyoto Encyclopedia of Genes and Genomes” (KEGG) database, which is accessed through a function that maps the UniProt IDs to corresponding pathways. 

#### Custom ontology based protein enrichment analysis
For custom ontologies the protein enrichment analysis is done using the R bioconductor package piano \citep{Vaeremo2013}. In opposite to the pathway enrichment, piano maps proteins to a pre-selected gene collection set, which can be either downloaded from gsea-msigdb.org or custom made for the particular experiment and explains conditions, phenotype or other desired properties.
For statistical testing the above explained Fisher exact test is used.



